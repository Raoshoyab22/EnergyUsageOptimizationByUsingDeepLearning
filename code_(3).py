# -*- coding: utf-8 -*-
"""Code (3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GGm84vw4iceQjVlP0DBNxcSbjMRCsylW
"""

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import LSTM, GRU, Dense
from keras.preprocessing.sequence import TimeseriesGenerator
import numpy as np

# Load your dataset
from google.colab import drive
drive.mount('/content/drive')

# Read the CSV file from your drive
file_path = '/content/drive/My Drive/dataset_1.csv' # Update with your file path
df = pd.read_csv(file_path)

# Combine 'Date' and 'Time' columns into a single datetime column
df['Date Time'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])

# Set 'Date Time' as the index
df.set_index('Date Time', inplace=True)

# Drop the original 'Date' and 'Time' columns
df.drop(['Date', 'Time'], axis=1, inplace=True)

# Separate the target variables
target_vars = ['Air Conditioner', 'Refrigerator', 'Washing Machine']
target_data = df[target_vars]


# Scaling the target data
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(target_data)

# Define sequence length
sequence_length = 24

# Splitting the dataset
train_size = int(len(scaled_data) * 6 / 7)  # Train on the first 6 months (approximately 86% of the data)
train_data = scaled_data[:train_size]
test_data = scaled_data[train_size:]  # Test on the 7th month

# Create generators
train_generator = TimeseriesGenerator(train_data, train_data, length=sequence_length, batch_size=32)
test_generator = TimeseriesGenerator(test_data, test_data, length=sequence_length, batch_size=32)

# Define LSTM model
def create_lstm_model(num_features):
    model = Sequential()
    model.add(LSTM(50, activation='relu', input_shape=(sequence_length, num_features)))
    model.add(Dense(num_features))
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

# Define GRU model
def create_gru_model(num_features):
    model = Sequential()
    model.add(GRU(50, activation='relu', input_shape=(sequence_length, num_features)))
    model.add(Dense(num_features))
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

num_features = scaled_data.shape[1]

# Create and train models
lstm_model = create_lstm_model(num_features)
gru_model = create_gru_model(num_features)
lstm_model.fit(train_generator, epochs=15, verbose=1)
gru_model.fit(train_generator, epochs=15, verbose=1)

# Ensemble Prediction with weighted average
# Assigning higher weight (0.6) to LSTM and lower weight (0.4) to GRU based on intuition
lstm_predictions = lstm_model.predict(test_generator)
gru_predictions = gru_model.predict(test_generator)
ensemble_predictions = (lstm_predictions * 0.6) + (gru_predictions * 0.4)

# Inverse transform the predictions
ensemble_predictions = scaler.inverse_transform(ensemble_predictions)
test_data_inverse = scaler.inverse_transform(test_data)

# Trim the ensemble predictions and test data to 1 month
num_days = 30
num_hours = num_days * 24
ensemble_predictions = ensemble_predictions[:num_hours]
test_data_inverse = test_data_inverse[:num_hours]

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
file_path = '/content/drive/My Drive/dataset_1.csv'  # Update with your file path
dataset = pd.read_csv(file_path)
dataset['Date Time'] = pd.to_datetime(dataset['Date'] + ' ' + dataset['Time'])
dataset.set_index('Date Time', inplace=True)
dataset.drop(['Date', 'Time'], axis=1, inplace=True)

# Separate the target variables
target_vars = ['Air Conditioner', 'Refrigerator', 'Washing Machine']

# Visualize the energy consumption patterns
for var in target_vars:
    plt.figure(figsize=(12, 6))
    plt.plot(dataset[var])
    plt.title(f'Energy Consumption Pattern for {var}')
    plt.xlabel('Time')
    plt.ylabel('Energy Consumption')
    plt.show()

# Calculate the hourly mean energy consumption
hourly_means = dataset[target_vars].groupby(dataset.index.hour).mean()

# Plot the hourly mean energy consumption
plt.figure(figsize=(12, 6))
for var in target_vars:
    plt.plot(hourly_means[var], label=var)
plt.title('Hourly Mean Energy Consumption')
plt.xlabel('Hour of the Day')
plt.ylabel('Energy Consumption')
plt.legend()
plt.show()

# Calculate the daily mean energy consumption
daily_means = dataset[target_vars].groupby(dataset.index.date).mean()

# Plot the daily mean energy consumption
plt.figure(figsize=(12, 6))
for var in target_vars:
    sns.lineplot(data=daily_means[var], label=var)
plt.title('Daily Mean Energy Consumption')
plt.xlabel('Date')
plt.ylabel('Energy Consumption')
plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Load the dataset
file_path = '/content/drive/My Drive/dataset_1.csv'  # Update with your file path
dataset = pd.read_csv(file_path)

# Combine Date and Time columns into a single Date Time column
dataset['Date Time'] = pd.to_datetime(dataset['Date'] + ' ' + dataset['Time'])
dataset.set_index('Date Time', inplace=True)
dataset.drop(['Date', 'Time'], axis=1, inplace=True)

# Separate the target variables
target_vars = ['Air Conditioner', 'Refrigerator', 'Washing Machine']

# Function to find trend hours
def find_trend_hours(df, appliance):
    # Group by hour and calculate the average usage
    hourly_usage = df.groupby(df.index.hour)[appliance].mean().reset_index()
    hourly_usage.columns = ['hour', appliance]  # Rename columns

    # Sort the hours by average usage in descending order
    sorted_usage = hourly_usage.sort_values(by=appliance, ascending=False).reset_index(drop=True)

    # Start with the highest usage hour
    trend_hours = [int(sorted_usage.iloc[0]['hour'])]
    previous_usage = sorted_usage.iloc[0][appliance]

    # Find subsequent hours with lower usage
    for i in range(1, len(sorted_usage)):
        current_usage = sorted_usage.iloc[i][appliance]
        if current_usage < previous_usage:
            trend_hours.append(int(sorted_usage.iloc[i]['hour']))
            previous_usage = current_usage

    return trend_hours

# Example usage
trend_hours_dict = {}
for appliance in target_vars:
    trend_hours = find_trend_hours(dataset, appliance)
    trend_hours_dict[appliance] = trend_hours
    print(f"Trend hours from highest to lower usage for {appliance}: {trend_hours}")

# Visualizing the trend hours for each appliance
for appliance in target_vars:
    hourly_usage = dataset.groupby(dataset.index.hour)[appliance].mean().reset_index()
    hourly_usage.columns = ['hour', appliance]  # Rename columns

    # Reorder based on the trend hours
    hourly_usage = hourly_usage.set_index('hour').loc[trend_hours_dict[appliance]].reset_index()

    plt.figure(figsize=(12, 6))
    plt.scatter(hourly_usage['hour'], hourly_usage[appliance], marker='o', label=appliance)
    plt.title(f'Trend Hours for {appliance}')
    plt.xlabel('Hour of the Day')
    plt.ylabel('Average Energy Consumption')
    plt.xticks(hourly_usage['hour'])  # Ensure all trend hours are shown on x-axis
    plt.grid(True)
    plt.legend()
    plt.show()

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt
from google.colab import drive

drive.mount('/content/drive')

# Load the dataset
file_path = '/content/drive/My Drive/dataset_1.csv'

# Load the dataset
dataset = pd.read_csv(file_path)

# Combine Date and Time columns
dataset['Date Time'] = pd.to_datetime(dataset['Date'] + ' ' + dataset['Time'])
dataset.set_index('Date Time', inplace=True)
dataset.drop(['Date', 'Time'], axis=1, inplace=True)

# Add hour column for later use
dataset['hour'] = dataset.index.hour

# Calculate the cost for each appliance with random variations
def calculate_cost(df, appliance):
    df['cost'] = df[appliance] * (1 + np.random.uniform(-0.2, 0.2))  # Apply random variation to all hours
    return df

# Function to forecast costs and recommend avoidance hours for the air conditioner
def forecast_and_recommend_ac(df, appliance, num_hours_to_avoid=3):
    df = calculate_cost(df.copy(), appliance)
    # Prepare the dataset for the model
    X = df[['hour', 'Temperature']]
    y = df['cost']
    # Train a Random Forest model
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X, y)
    # Predict costs for all hours
    hourly_temps = df.groupby('hour')['Temperature'].mean().reset_index()
    hourly_temps['predicted_cost'] = model.predict(hourly_temps[['hour', 'Temperature']])
    # Find the highest cost hours to avoid
    avoid_hours = hourly_temps.nlargest(num_hours_to_avoid, 'predicted_cost')['hour'].tolist()

    # Visualize predicted costs
    plt.figure(figsize=(10, 6))
    plt.plot(hourly_temps['hour'], hourly_temps['predicted_cost'])
    plt.xlabel('Hour of the Day')
    plt.ylabel('Predicted Cost')
    plt.title(f'Predicted Costs for {appliance}')
    plt.show()

    return avoid_hours, hourly_temps['predicted_cost'].sum()

# Function to forecast costs and recommend avoidance hours for other appliances
def forecast_and_recommend_others(df, appliance, num_hours_to_avoid=3):
    df = calculate_cost(df.copy(), appliance)
    # Prepare the dataset for the model
    X = df[['hour']]
    y = df['cost']
    # Train a Random Forest model
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X, y)
    # Predict costs for all hours
    hourly_usage = df.groupby('hour')[appliance].mean().reset_index()
    hourly_usage['predicted_cost'] = model.predict(hourly_usage[['hour']])
    # Find the highest cost hours to avoid
    avoid_hours = hourly_usage.nlargest(num_hours_to_avoid, 'predicted_cost')['hour'].tolist()

    # Visualize predicted costs
    plt.figure(figsize=(10, 6))
    plt.plot(hourly_usage['hour'], hourly_usage['predicted_cost'])
    plt.xlabel('Hour of the Day')
    plt.ylabel('Predicted Cost')
    plt.title(f'Predicted Costs for {appliance}')
    plt.show()

    return avoid_hours, hourly_usage['predicted_cost'].sum()

# Example usage
appliances = ['Air Conditioner', 'Refrigerator', 'Washing Machine']
for appliance in appliances:
    if appliance == 'Air Conditioner':
        avoid_hours, total_predicted_cost = forecast_and_recommend_ac(dataset, appliance)
    else:
        avoid_hours, total_predicted_cost = forecast_and_recommend_others(dataset, appliance)
    print(f"Hours to avoid using {appliance}: {avoid_hours}")
    print(f"Total predicted cost for {appliance}: {total_predicted_cost:.2f}")

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Load the dataset
file_path = '/content/drive/My Drive/dataset_1.csv'

# Load the dataset
dataset = pd.read_csv(file_path)

# Combine Date and Time columns
dataset['Date Time'] = pd.to_datetime(dataset['Date'] + ' ' + dataset['Time'])
dataset.set_index('Date Time', inplace=True)
dataset.drop(['Date', 'Time'], axis=1, inplace=True)

# Add hour column for later use
dataset['hour'] = dataset.index.hour

# Ensure 'Temperature' column exists and print the dataset structure
print("Dataset columns:", dataset.columns)
print(dataset.head())

# Calculate the cost for each appliance with random variations
def calculate_cost(df, appliance):
    df['cost'] = df[appliance] * (1 + np.random.uniform(-0.2, 0.2))  # Apply random variation to all hours
    return df

# Function to forecast costs and recommend avoidance hours for the air conditioner
def forecast_and_recommend_ac(df, appliance, num_hours_to_avoid=3):
    df = calculate_cost(df.copy(), appliance)

    # Ensure 'Temperature' column exists in the dataframe
    if 'Temperature' not in df.columns:
        print("Error: 'Temperature' column not found in the dataframe")
        return [], 0

    # Prepare the dataset for the model
    X = df[['hour', 'Temperature']]
    y = df['cost']

    # Train a Random Forest model
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X, y)

    # Predict costs for all hours
    hourly_temps = df.groupby('hour')['Temperature'].mean().reset_index()
    hourly_temps['predicted_cost'] = model.predict(hourly_temps[['hour', 'Temperature']])

    # Find the highest cost hours to avoid
    avoid_hours = hourly_temps.nlargest(num_hours_to_avoid, 'predicted_cost')['hour'].tolist()

    # Visualize predicted costs
    plt.figure(figsize=(10, 6))
    plt.plot(hourly_temps['hour'], hourly_temps['predicted_cost'])
    plt.xlabel('Hour of the Day')
    plt.ylabel('Predicted Cost')
    plt.title(f'Predicted Costs for {appliance}')
    plt.show()

    return avoid_hours, hourly_temps['predicted_cost'].sum()

# Function to forecast costs and recommend avoidance hours for other appliances
def forecast_and_recommend_others(df, appliance, num_hours_to_avoid=3):
    df = calculate_cost(df.copy(), appliance)

    # Prepare the dataset for the model
    X = df[['hour']]
    y = df['cost']

    # Train a Random Forest model
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X, y)

    # Predict costs for all hours
    hourly_usage = df.groupby('hour')[appliance].mean().reset_index()
    hourly_usage['predicted_cost'] = model.predict(hourly_usage[['hour']])

    # Find the highest cost hours to avoid
    avoid_hours = hourly_usage.nlargest(num_hours_to_avoid, 'predicted_cost')['hour'].tolist()

    # Visualize predicted costs
    plt.figure(figsize=(10, 6))
    plt.plot(hourly_usage['hour'], hourly_usage['predicted_cost'])
    plt.xlabel('Hour of the Day')
    plt.ylabel('Predicted Cost')
    plt.title(f'Predicted Costs for {appliance}')
    plt.show()

    return avoid_hours, hourly_usage['predicted_cost'].sum()

# Example usage
appliances = ['Air Conditioner', 'Refrigerator', 'Washing Machine']
for appliance in appliances:
    if appliance == 'Air Conditioner':
        avoid_hours, total_predicted_cost = forecast_and_recommend_ac(dataset, appliance)
    else:
        avoid_hours, total_predicted_cost = forecast_and_recommend_others(dataset, appliance)
    print(f"Hours to avoid using {appliance}: {avoid_hours}")
    print(f"Total predicted cost for {appliance}: {total_predicted_cost:.2f}")

import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error

# Plot and evaluate for each target variable
for i, var in enumerate(target_vars):
    plt.figure(figsize=(12, 6))
    plt.plot(test_data_inverse[:, i], label='Actual')
    plt.plot(ensemble_predictions[:, i], label='Ensemble Prediction')
    plt.xlabel('Time')
    plt.ylabel(var)
    plt.title(f'Actual vs. Ensemble Prediction for {var}')
    plt.legend()
    plt.show()

    # Ensure both arrays have the same length
    min_length = min(len(test_data_inverse[:, i]), len(ensemble_predictions[:, i]))
    test_data_inverse_var = test_data_inverse[:min_length, i]
    ensemble_predictions_var = ensemble_predictions[:min_length, i]

    # Calculate RMSE on the original scale
    rmse = np.sqrt(mean_squared_error(test_data_inverse_var, ensemble_predictions_var))
    print(f"Root Mean Squared Error (RMSE) for {var}: {rmse}")

    # Calculate R^2 Score on the original scale
    r2 = r2_score(test_data_inverse_var, ensemble_predictions_var)
    print(f"R-squared (R^2) for {var}: {r2}")

    # Calculate MAE on the original scale
    mae = mean_absolute_error(test_data_inverse_var, ensemble_predictions_var)
    print(f"Mean Absolute Error (MAE) for {var}: {mae}")

    # Calculate MAPE - Mean Absolute Percentage Error on the original scale
    # To avoid division by zero, make sure there are no zero values in the test_data_inverse_var
    non_zero_indices = test_data_inverse_var != 0
    mape = np.mean(np.abs((test_data_inverse_var[non_zero_indices] - ensemble_predictions_var[non_zero_indices]) / test_data_inverse_var[non_zero_indices])) * 100
    print(f"Mean Absolute Percentage Error (MAPE) for {var}: {mape}")
    print()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.ensemble import RandomForestRegressor
from keras.models import Sequential
from keras.layers import LSTM, GRU, Dense
from keras.preprocessing.sequence import TimeseriesGenerator
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Load your dataset
file_path = '/content/drive/My Drive/dataset_1.csv'  # Update with your file path
df = pd.read_csv(file_path)

# Combine 'Date' and 'Time' columns into a single datetime column
df['Date Time'] = pd.to_datetime(df['Date'] + ' ' + df['Time'], format='%Y-%m-%d %H:%M:%S')
df.set_index('Date Time', inplace=True)
df.drop(['Date', 'Time'], axis=1, inplace=True)

# Separate the target variables
target_vars = ['Air Conditioner', 'Refrigerator', 'Washing Machine']
target_data = df[target_vars]

# Scaling the target data
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(target_data)

# Define sequence length
sequence_length = 24

# Splitting the dataset
train_size = int(len(scaled_data) * 6 / 7)  # Train on the first 6 months (approximately 86% of the data)
train_data = scaled_data[:train_size]
test_data = scaled_data[train_size:]  # Test on the 7th month

# Create generators
train_generator = TimeseriesGenerator(train_data, train_data, length=sequence_length, batch_size=32)
test_generator = TimeseriesGenerator(test_data, test_data, length=sequence_length, batch_size=32)

# Define LSTM model
def create_lstm_model(num_features):
    model = Sequential()
    model.add(LSTM(50, activation='relu', input_shape=(sequence_length, num_features)))
    model.add(Dense(num_features))
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

# Define GRU model
def create_gru_model(num_features):
    model = Sequential()
    model.add(GRU(50, activation='relu', input_shape=(sequence_length, num_features)))
    model.add(Dense(num_features))
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

num_features = scaled_data.shape[1]

# Create and train models
lstm_model = create_lstm_model(num_features)
gru_model = create_gru_model(num_features)
lstm_model.fit(train_generator, epochs=15, verbose=1)
gru_model.fit(train_generator, epochs=15, verbose=1)

# Ensemble Prediction with weighted average
lstm_predictions = lstm_model.predict(test_generator)
gru_predictions = gru_model.predict(test_generator)
ensemble_predictions = (lstm_predictions * 0.6) + (gru_predictions * 0.4)

# Inverse transform the predictions
ensemble_predictions = scaler.inverse_transform(ensemble_predictions)
test_data_inverse = scaler.inverse_transform(test_data)

# Trim the ensemble predictions and test data to 1 month
num_days = 30
num_hours = num_days * 24
ensemble_predictions = ensemble_predictions[:num_hours]
test_data_inverse = test_data_inverse[:num_hours]

# Plot and evaluate for each target variable
for i, var in enumerate(target_vars):
    plt.figure(figsize=(12, 6))
    plt.plot(test_data_inverse[:, i], label='Actual')
    plt.plot(ensemble_predictions[:, i], label='Ensemble Prediction')
    plt.xlabel('Time')
    plt.ylabel(var)
    plt.title(f'Actual vs. Ensemble Prediction for {var}')
    plt.legend()
    plt.show()

    # Ensure both arrays have the same length
    min_length = min(len(test_data_inverse[:, i]), len(ensemble_predictions[:, i]))
    test_data_inverse_var = test_data_inverse[:min_length, i]
    ensemble_predictions_var = ensemble_predictions[:min_length, i]

    # Calculate RMSE on the original scale
    rmse = np.sqrt(mean_squared_error(test_data_inverse_var, ensemble_predictions_var))
    print(f"Root Mean Squared Error (RMSE) for {var}: {rmse}")

    # Calculate R^2 Score on the original scale
    r2 = r2_score(test_data_inverse_var, ensemble_predictions_var)
    print(f"R-squared (R^2) for {var}: {r2}")

    # Calculate MAE on the original scale
    mae = mean_absolute_error(test_data_inverse_var, ensemble_predictions_var)
    print(f"Mean Absolute Error (MAE) for {var}: {mae}")

    # Calculate MAPE - Mean Absolute Percentage Error on the original scale
    non_zero_indices = test_data_inverse_var != 0
    mape = np.mean(np.abs((test_data_inverse_var[non_zero_indices] - ensemble_predictions_var[non_zero_indices]) / test_data_inverse_var[non_zero_indices])) * 100
    print(f"Mean Absolute Percentage Error (MAPE) for {var}: {mape}")
    print()

# Visualize the energy consumption patterns
for var in target_vars:
    plt.figure(figsize=(12, 6))
    plt.plot(df[var])
    plt.title(f'Energy Consumption Pattern for {var}')
    plt.xlabel('Time')
    plt.ylabel('Energy Consumption')
    plt.show()

# Calculate the hourly mean energy consumption
hourly_means = df[target_vars].groupby(df.index.hour).mean()

# Plot the hourly mean energy consumption
plt.figure(figsize=(12, 6))
for var in target_vars:
    plt.plot(hourly_means[var], label=var)
plt.title('Hourly Mean Energy Consumption')
plt.xlabel('Hour of the Day')
plt.ylabel('Energy Consumption')
plt.legend()
plt.show()

# Calculate the daily mean energy consumption
daily_means = df[target_vars].groupby(df.index.date).mean()

# Plot the daily mean energy consumption
plt.figure(figsize=(12, 6))
for var in target_vars:
    sns.lineplot(data=daily_means[var], label=var)
plt.title('Daily Mean Energy Consumption')
plt.xlabel('Date')
plt.ylabel('Energy Consumption')
plt.legend()
plt.show()

# Function to find trend hours
def find_trend_hours(df, appliance):
    hourly_usage = df.groupby(df.index.hour)[appliance].mean().reset_index()
    hourly_usage.columns = ['hour', appliance]
    sorted_usage = hourly_usage.sort_values(by=appliance, ascending=False).reset_index(drop=True)
    trend_hours = [int(sorted_usage.iloc[0]['hour'])]
    previous_usage = sorted_usage.iloc[0][appliance]

    for i in range(1, len(sorted_usage)):
        current_usage = sorted_usage.iloc[i][appliance]
        if current_usage < previous_usage:
            trend_hours.append(int(sorted_usage.iloc[i]['hour']))
            previous_usage = current_usage

    return trend_hours

# Example usage
trend_hours_dict = {}
for appliance in target_vars:
    trend_hours = find_trend_hours(df, appliance)
    trend_hours_dict[appliance] = trend_hours
    print(f"Trend hours from highest to lower usage for {appliance}: {trend_hours}")

# Visualizing the trend hours for each appliance
for appliance in target_vars:
    hourly_usage = df.groupby(df.index.hour)[appliance].mean().reset_index()
    hourly_usage.columns = ['hour', appliance]
    hourly_usage = hourly_usage.set_index('hour').loc[trend_hours_dict[appliance]].reset_index()

    plt.figure(figsize=(12, 6))
    plt.scatter(hourly_usage['hour'], hourly_usage[appliance], marker='o', label=appliance)
    plt.title(f'Trend Hours for {appliance}')
    plt.xlabel('Hour of the Day')
    plt.ylabel('Average Energy Consumption')
    plt.xticks(hourly_usage['hour'])
    plt.grid(True)
    plt.legend()
    plt.show()

# Calculate the cost for each appliance with random variations
def calculate_cost(df, appliance):
    df['cost'] = df[appliance] * (1 + np.random.uniform(-0.2, 0.2))
    return df

# Function to forecast costs and recommend avoidance hours for the air conditioner
def forecast_and_recommend_ac(df, appliance, num_hours_to_avoid=3):
    df = calculate_cost(df.copy(), appliance)
    df['hour'] = df.index.hour
    X = df[['hour', 'Temperature']]
    y = df['cost']
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X, y)
    hourly_temps = df.groupby('hour')['Temperature'].mean().reset_index()
    hourly_temps['predicted_cost'] = model.predict(hourly_temps[['hour', 'Temperature']])
    avoid_hours = hourly_temps.nlargest(num_hours_to_avoid, 'predicted_cost')['hour'].tolist()

    plt.figure(figsize=(10, 6))
    plt.plot(hourly_temps['hour'], hourly_temps['predicted_cost'])
    plt.xlabel('Hour of the Day')
    plt.ylabel('Predicted Cost')
    plt.title(f'Predicted Costs for {appliance}')
    plt.show()

    return avoid_hours, hourly_temps['predicted_cost'].sum()

# Function to forecast costs and recommend avoidance hours for other appliances
def forecast_and_recommend_others(df, appliance, num_hours_to_avoid=3):
    df = calculate_cost(df.copy(), appliance)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.ensemble import RandomForestRegressor
from keras.models import Sequential
from keras.layers import LSTM, GRU, Dense
from keras.preprocessing.sequence import TimeseriesGenerator
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Load your dataset
file_path = '/content/drive/My Drive/dataset_1.csv'  # Update with your file path
df = pd.read_csv(file_path)

# Combine 'Date' and 'Time' columns into a single datetime column
df['Date Time'] = pd.to_datetime(df['Date'] + ' ' + df['Time'], format='%Y-%m-%d %H:%M:%S')
df.set_index('Date Time', inplace=True)
df.drop(['Date', 'Time'], axis=1, inplace=True)

# Separate the target variables
target_vars = ['Air Conditioner', 'Refrigerator', 'Washing Machine']
target_data = df[target_vars]

# Scaling the target data
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(target_data)

# Define sequence length
sequence_length = 24

# Splitting the dataset
train_size = int(len(scaled_data) * 6 / 7)  # Train on the first 6 months (approximately 86% of the data)
train_data = scaled_data[:train_size]
test_data = scaled_data[train_size:]  # Test on the 7th month

# Create generators
train_generator = TimeseriesGenerator(train_data, train_data, length=sequence_length, batch_size=32)
test_generator = TimeseriesGenerator(test_data, test_data, length=sequence_length, batch_size=32)

# Define LSTM model
def create_lstm_model(num_features):
    model = Sequential()
    model.add(LSTM(50, activation='relu', input_shape=(sequence_length, num_features)))
    model.add(Dense(num_features))
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

# Define GRU model
def create_gru_model(num_features):
    model = Sequential()
    model.add(GRU(50, activation='relu', input_shape=(sequence_length, num_features)))
    model.add(Dense(num_features))
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

num_features = scaled_data.shape[1]

# Create and train models
lstm_model = create_lstm_model(num_features)
gru_model = create_gru_model(num_features)
lstm_model.fit(train_generator, epochs=15, verbose=1)
gru_model.fit(train_generator, epochs=15, verbose=1)

# Ensemble Prediction with weighted average
lstm_predictions = lstm_model.predict(test_generator)
gru_predictions = gru_model.predict(test_generator)
ensemble_predictions = (lstm_predictions * 0.6) + (gru_predictions * 0.4)

# Inverse transform the predictions
ensemble_predictions = scaler.inverse_transform(ensemble_predictions)
test_data_inverse = scaler.inverse_transform(test_data)

# Trim the ensemble predictions and test data to 1 month
num_days = 30
num_hours = num_days * 24
ensemble_predictions = ensemble_predictions[:num_hours]
test_data_inverse = test_data_inverse[:num_hours]

# Plot and evaluate for each target variable
for i, var in enumerate(target_vars):
    plt.figure(figsize=(12, 6))
    plt.plot(test_data_inverse[:, i], label='Actual')
    plt.plot(ensemble_predictions[:, i], label='Ensemble Prediction')
    plt.xlabel('Time')
    plt.ylabel(var)
    plt.title(f'Actual vs. Ensemble Prediction for {var}')
    plt.legend()
    plt.show()

    # Ensure both arrays have the same length
    min_length = min(len(test_data_inverse[:, i]), len(ensemble_predictions[:, i]))
    test_data_inverse_var = test_data_inverse[:min_length, i]
    ensemble_predictions_var = ensemble_predictions[:min_length, i]

    # Calculate RMSE on the original scale
    rmse = np.sqrt(mean_squared_error(test_data_inverse_var, ensemble_predictions_var))
    print(f"Root Mean Squared Error (RMSE) for {var}: {rmse}")

    # Calculate R^2 Score on the original scale
    r2 = r2_score(test_data_inverse_var, ensemble_predictions_var)
    print(f"R-squared (R^2) for {var}: {r2}")

    # Calculate MAE on the original scale
    mae = mean_absolute_error(test_data_inverse_var, ensemble_predictions_var)
    print(f"Mean Absolute Error (MAE) for {var}: {mae}")

    # Calculate MAPE - Mean Absolute Percentage Error on the original scale
    non_zero_indices = test_data_inverse_var != 0
    mape = np.mean(np.abs((test_data_inverse_var[non_zero_indices] - ensemble_predictions_var[non_zero_indices]) / test_data_inverse_var[non_zero_indices])) * 100
    print(f"Mean Absolute Percentage Error (MAPE) for {var}: {mape}")
    print()

# Visualize the energy consumption patterns
for var in target_vars:
    plt.figure(figsize=(12, 6))
    plt.plot(df[var])
    plt.title(f'Energy Consumption Pattern for {var}')
    plt.xlabel('Time')
    plt.ylabel('Energy Consumption')
    plt.show()

# Calculate the hourly mean energy consumption
hourly_means = df[target_vars].groupby(df.index.hour).mean()

# Plot the hourly mean energy consumption
plt.figure(figsize=(12, 6))
for var in target_vars:
    plt.plot(hourly_means[var], label=var)
plt.title('Hourly Mean Energy Consumption')
plt.xlabel('Hour of the Day')
plt.ylabel('Energy Consumption')
plt.legend()
plt.show()

# Calculate the daily mean energy consumption
daily_means = df[target_vars].groupby(df.index.date).mean()

# Plot the daily mean energy consumption
plt.figure(figsize=(12, 6))
for var in target_vars:
    sns.lineplot(data=daily_means[var], label=var)
plt.title('Daily Mean Energy Consumption')
plt.xlabel('Date')
plt.ylabel('Energy Consumption')
plt.legend()
plt.show()

# Function to find trend hours
def find_trend_hours(df, appliance):
    hourly_usage = df.groupby(df.index.hour)[appliance].mean().reset_index()
    hourly_usage.columns = ['hour', appliance]
    sorted_usage = hourly_usage.sort_values(by=appliance, ascending=False).reset_index(drop=True)
    trend_hours = [int(sorted_usage.iloc[0]['hour'])]
    previous_usage = sorted_usage.iloc[0][appliance]

    for i in range(1, len(sorted_usage)):
        current_usage = sorted_usage.iloc[i][appliance]
        if current_usage < previous_usage:
            trend_hours.append(int(sorted_usage.iloc[i]['hour']))
            previous_usage = current_usage

    return trend_hours

# Example usage
trend_hours_dict = {}
for appliance in target_vars:
    trend_hours = find_trend_hours(df, appliance)
    trend_hours_dict[appliance] = trend_hours
    print(f"Trend hours from highest to lower usage for {appliance}: {trend_hours}")

# Visualizing the trend hours for each appliance
for appliance in target_vars:
    hourly_usage = df.groupby(df.index.hour)[appliance].mean().reset_index()
    hourly_usage.columns = ['hour', appliance]
    hourly_usage = hourly_usage.set_index('hour').loc[trend_hours_dict[appliance]].reset_index()

    plt.figure(figsize=(12, 6))
    plt.scatter(hourly_usage['hour'], hourly_usage[appliance], marker='o', label=appliance)
    plt.title(f'Trend Hours for {appliance}')
    plt.xlabel('Hour of the Day')
    plt.ylabel('Average Energy Consumption')
    plt.xticks(hourly_usage['hour'])
    plt.grid(True)
    plt.legend()
    plt.show()

# Calculate the cost for each appliance with random variations
def calculate_cost(df, appliance):
    df['cost'] = df[appliance] * (1 + np.random.uniform(-0.2, 0.2))
    return df

# Function to forecast costs and recommend avoidance hours for the air conditioner
def forecast_and_recommend_ac(df, appliance, num_hours_to_avoid=3):
    df = calculate_cost(df.copy(), appliance)
    df['hour'] = df.index.hour
    X = df[['hour', 'Temperature']]
    y = df['cost']
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X, y)
    hourly_temps = df.groupby('hour')['Temperature'].mean().reset_index()
    hourly_temps['predicted_cost'] = model.predict(hourly_temps[['hour', 'Temperature']])
    avoid_hours = hourly_temps.nlargest(num_hours_to_avoid, 'predicted_cost')['hour'].tolist()

    plt.figure(figsize=(10, 6))
    plt.plot(hourly_temps['hour'], hourly_temps['predicted_cost'])
    plt.xlabel('Hour of the Day')
    plt.ylabel('Predicted Cost')
    plt.title(f'Predicted Costs for {appliance}')
    plt.show()

    return avoid_hours, hourly_temps['predicted_cost'].sum()

# Function to forecast costs and recommend avoidance hours for other appliances
def forecast_and_recommend_others(df, appliance, num_hours_to_avoid=3):
    df = calculate_cost(df.copy(), appliance)